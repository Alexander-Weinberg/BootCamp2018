Notes_HPC

LOGIN: ssh -X weinberga@midway1.rcc.uchicago.edu

CLONE REPO: git clone https://github.com/sischei/OSM2018
1. develop on your comp
2. upload to github
3. clone onto the supercomputer

1. load packages into your enviornment
module avail - which are available
module list - which do you have loaded
module load matlab - load matlab
module unload matlab - unload matlab

2. compile:  g++ helloworld.cpp
or
make -f makefile_helloworld_cpp
for fortran it is gfortran

3. SLURM
it allocates for you, also keeps track of stuff for you
sbatch submit_helloworld.sh
squeue -u weinberga
scancel JOB_ID

###################
# Submiting a task
##################
SBATCH --ntasks=1              # how many CPUS
SBATCH --time=01:00:00         # walltime requested
SBATCH --output=slurm_test.out # output file
SBATCH --error=slurm_test.err  # error

SBATCH --mail-type=BEGIN,END,DAIL

################################
DAY 2
################################

Parallel Computing, openmp

1. No point in parallelizing shit code
2. How to optimize code?

- profiling will find the hot-spots of your code


Priorities:
 1. correct
 2. maintainable
 3. fast

Common sense speed-ups:
1. do less idle computation
2. avoid branching (ifelse statements)
3. access data smartly
    python, cpp = [0,0], [0,1], [0,2], [0,3]
    fortran = [1,1], [2,1], [3,1], [4,1]
    15x speedup
4. Vectorization
    use pre-built libraries

############################
DAY 3
############################

1. shared and private things
2. use those prebuilt structures to implement

mpi-forum.org
3. all variables are private/local but if you want to use them in parallel need to pass information

4. Workflow
a) call mpi enviornment
b)

communicators = collection of all the processes you have
ranks =   unique identifier

dont use more mpi's than cores you have

########################################
# Fourth class
########################################

1. spllit the mpi communicator, scatter a specific range of the value function
2. distrubute those subsequence with OpenMP
3. cd OSM2018/project2/osmlab
